var documenterSearchIndex = {"docs":
[{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"using MLPGradientFlow, Random, Optimisers\n\nRandom.seed!(1)\n\ninput = randn(2, 10_000)\nteacher = TeacherNet(; layers = ((5, softplus, false), (1, identity, false)), input)\ntarget = teacher(input)\n\nnet = Net(; layers = ((4, softplus, false), (1, identity, false)),\n            input, target)\np = random_params(net)\n\nres_gradientflow = train(net, p, maxT = 30,\n                         maxiterations_optim = 0,\n                         n_samples_trajectory = 10^4)\n","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"Let us compare to training without and with minibatches and gradient descent.","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"res_descent_fullbatch = train(net, p, alg = Descent(eta = 1e-1),\n                              maxiterations_ode = 10^8,\n                              maxtime_ode = 30, maxiterations_optim = 0,\n                              n_samples_trajectory = 10^3)","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"res_descent = train(net, p, alg = Descent(eta = 1e-1), batchsize = 100,\n                    maxiterations_ode = 10^8,\n                    maxtime_ode = 20, maxiterations_optim = 0,\n                    n_samples_trajectory = 10^3)\n","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"Not surprisingly, gradient descent takes more time than gradient flow (which uses second order information), and therefore does not find a point of equally low loss and gradient as gradient flow.","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"tdb, ttb, _ = MLPGradientFlow.trajectory_distance(res_descent_fullbatch, res_gradientflow)\ntd, tt, _ = MLPGradientFlow.trajectory_distance(res_descent, res_gradientflow)\n\nusing CairoMakie\n\nf = Figure()\nax = Axis(f[1, 1], ylabel = \"distance\", yscale = Makie.pseudolog10, xscale = Makie.pseudolog10, xlabel = \"time\")\nlines!(ax, ttb, tdb, label = \"full batch\")\nlines!(ax, tt, td, label = \"batchsize = 100\")\naxislegend(ax)\nf\nsave(\"trajectory_distance1.png\", ans); # hide","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"(Image: )","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"Gradient descent stays close to gradient flow, both in full batch mode and with minibatches of size 100.","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"res_adam_fullbatch = train(net, p, alg = Adam(),\n                           maxtime_ode = 20, maxiterations_optim = 0,\n                           n_samples_trajectory = 10^3)","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"res_adam = train(net, p, alg = Adam(), batchsize = 100, maxiterations_ode = 10^8,\n                 maxtime_ode = 20, maxiterations_optim = 0,\n                 n_samples_trajectory = 10^3)","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"tdb, ttb, _ = MLPGradientFlow.trajectory_distance(res_adam_fullbatch, res_gradientflow)\ntd, tt, _ = MLPGradientFlow.trajectory_distance(res_adam, res_gradientflow)\n\nusing CairoMakie\n\nf = Figure()\nax = Axis(f[1, 1], ylabel = \"distance\", yscale = Makie.pseudolog10, xlabel = \"trajectory steps\")\nlines!(ax, 1:length(tdb), tdb, label = \"full batch\")\nlines!(ax, 1:length(td), td, label = \"batchsize = 100\")\naxislegend(ax, position = :rb)\nf\nsave(\"trajectory_distance2.png\", ans); # hide","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"(Image: )","category":"page"},{"location":"sgd/","page":"Comparison of Gradient Flow to SGD","title":"Comparison of Gradient Flow to SGD","text":"This is not the case for Adam which uses effectively different timescales for the different parameters.","category":"page"},{"location":"teacherstudent/","page":"Teacher student setup","title":"Teacher student setup","text":"using MLPGradientFlow, Random\n\nRandom.seed!(71)\n\nteacher = TeacherNet(layers = ((5, g, false), (1, identity, false)), Din = 2,\n                     p = params((randn(5, 2), nothing), (randn(1, 5), nothing)))\n\ninput = randn(2, 10_000)\ntarget = teacher(input)\n\nstudent = Net(; layers = ((5, g, false), (1, identity, false)),\n                input, target)\n\np = random_params(student)\n\nres = train(student, p,\n            maxtime_ode = 10, maxiterations_optim = 0,\n            n_samples_trajectory = 10^3)","category":"page"},{"location":"teacherstudent/","page":"Teacher student setup","title":"Teacher student setup","text":"Let us compare the solution found by the student to the teacher parameters:","category":"page"},{"location":"teacherstudent/","page":"Teacher student setup","title":"Teacher student setup","text":"p_res = params(res[\"x\"])\np_res.w1","category":"page"},{"location":"teacherstudent/","page":"Teacher student setup","title":"Teacher student setup","text":"teacher.p.w1","category":"page"},{"location":"teacherstudent/","page":"Teacher student setup","title":"Teacher student setup","text":"p_res.w2","category":"page"},{"location":"teacherstudent/","page":"Teacher student setup","title":"Teacher student setup","text":"teacher.p.w2","category":"page"},{"location":"teacherstudent/","page":"Teacher student setup","title":"Teacher student setup","text":"We see that the student perfectly reproduces the teacher up to permutation of the hidden neurons.","category":"page"},{"location":"activations/#Activation-Functions","page":"Activation Functions","title":"Activation Functions","text":"","category":"section"},{"location":"activations/","page":"Activation Functions","title":"Activation Functions","text":"using MLPGradientFlow, CairoMakie\n\nf = Figure()\nx = -5:1e-3:5\nfor (i, activation) in pairs((g, square, gelu, cube, softplus, relu, selu, silu, sigmoid, tanh_fast, sigmoid2, normal_cdf))\n    ax = Axis(f[(i-1) % 3, (i-1) ÷ 3], title = string(activation))\n    lines!(ax, x, activation.(x))\nend\nf\nsave(\"activations.png\", f); nothing # hide","category":"page"},{"location":"activations/","page":"Activation Functions","title":"Activation Functions","text":"(Image: )","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"There are two settings in which we can approximate gradient descent on the loss function integrated over an infinite amount of normally distributed input data, i.e. mathbb E_xleft(mathrmnet(p x) - y)^2right  with normally distributed x with mean 0 and standard deviation 1:","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"When the input dimension is sufficiently small (Din ≤ 2) such that we can use Gauss-Hermite quadrature. This works for arbitrary teacher functions and networks.\nWhen we have a single hidden layer teacher and student network. This is particularly fast, when using relu or erf-based activation functions, like normal_cdf or sigmoid2 (see Activation Functions).","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"We illustrate both cases in a 2D example.","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"using MLPGradientFlow, Statistics, Random\n\nRandom.seed!(12)\n\ninput = randn(2, 5_000)\nteacher = TeacherNet(; layers = ((5, sigmoid2, true), (1, identity, false)), input)\n\nstudent = Net(; layers = ((4, sigmoid2, true), (1, identity, false)),\n                input, target = teacher(input))\n\np = random_params(student)\n\nsample_losses = [let input = randn(2, 5_000), target = teacher(input)\n                    loss(student, p; input, target)\n                 end\n                 for _ in 1:10]\n(mean(sample_losses), std(sample_losses))","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"net_gh = gauss_hermite_net(teacher, student)\n\nloss(net_gh, p)","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"neti = NetI(teacher, student)\n\nloss(neti, p)","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"Let us compare training trajectories of the different networks. The timescales tauinv need to be adjusted for the dynamics based on the student or the Gauss-Hermite net_gh, such that the integration times match.","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"res_sample = train(student, p, tauinv = 1/MLPGradientFlow.n_samples(student),\n                   maxT = 10^4, maxiterations_optim = 0, n_samples_trajectory = 1000)","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"res_gh = train(net_gh, p, tauinv = 1/MLPGradientFlow.n_samples(net_gh), maxT = 10^4,\n               maxiterations_optim = 0, n_samples_trajectory = 1000)","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"resi = train(neti, p, maxT = 10^4,\n             maxiterations_optim = 0, n_samples_trajectory = 1000)","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"td1, tt1, _ = MLPGradientFlow.trajectory_distance(res_sample, resi)\ntd2, tt2, _ = MLPGradientFlow.trajectory_distance(res_gh, resi)\ntd3, tt3, _ = MLPGradientFlow.trajectory_distance(res_sample, res_gh)\n\nusing CairoMakie\n\nf = Figure()\nax = Axis(f[1, 1], ylabel = \"distance\", yscale = Makie.pseudolog10, xlabel = \"time\")\nlines!(ax, tt1, td1, label = \"sample vs. teacher-student\", linewidth = 3)\nlines!(ax, tt3, td3, label = \"sample vs. gauss-hermite\")\nlines!(ax, tt2, td2, label = \"gauss-hermite vs. teacher-student\")\naxislegend(ax, position = :lt)\nf\nsave(\"trajectory_normal.png\", ans); # hide","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"(Image: )","category":"page"},{"location":"normal/","page":"Standard Normal Input","title":"Standard Normal Input","text":"The Gauss-Hermite approximation and the teacher-student network based on symbolic integration of Gaussian integrals give almost indistinguishable results, whereas gradient flow on a finite number of normally distributed samples leads to slight deviations from the infinite data setting.","category":"page"},{"location":"tauinv/","page":"Separation of Timescales","title":"Separation of Timescales","text":"To experiment with different timescales for different parameters, we can use separate tauinv for each parameter. In the following example the biases change on a much faster timescale than the weights.","category":"page"},{"location":"tauinv/","page":"Separation of Timescales","title":"Separation of Timescales","text":"using MLPGradientFlow, Random\n\nRandom.seed!(14)\n\ninput = randn(2, 5_000)\nteacher = TeacherNet(; layers = ((5, sigmoid2, true), (1, identity, true)),\n                       p = params((randn(5, 2), randn(5)), (randn(1, 5), randn(1))),\n                       input)\n\nstudent = Net(; layers = ((4, sigmoid2, true), (1, identity, true)),\n                input, target = teacher(input))\n\np = random_params(student)\n\nneti = NetI(teacher, student)\n\nres_standard = train(neti, p; maxtime_ode = 10, maxiterations_optim = 0)","category":"page"},{"location":"tauinv/","page":"Separation of Timescales","title":"Separation of Timescales","text":"tauinv = zero(p) .+ 1\ntauinv.w1[:, end] .= 1e-4\ntauinv.w2[:, end] .= 1e-4\n\nres = train(neti, p; tauinv, maxtime_ode = 20, maxiterations_optim = 0)","category":"page"},{"location":"tauinv/","page":"Separation of Timescales","title":"Separation of Timescales","text":"We see that the two dynamics converge to different solutions.","category":"page"},{"location":"python/#Usage-from-Python","page":"Usage from Python","title":"Usage from Python","text":"","category":"section"},{"location":"python/#Installation","page":"Usage from Python","title":"Installation","text":"","category":"section"},{"location":"python/","page":"Usage from Python","title":"Usage from Python","text":"Install juliacall, e.g. pip install juliacall.","category":"page"},{"location":"python/","page":"Usage from Python","title":"Usage from Python","text":"from juliacall import Main as jl\njl.seval('using Pkg; Pkg.add(url = \"https://github.com/jbrea/MLPGradientFlow.jl.git\")')","category":"page"},{"location":"python/#Examples","page":"Usage from Python","title":"Examples","text":"","category":"section"},{"location":"python/","page":"Usage from Python","title":"Usage from Python","text":"import numpy as np\nimport juliacall as jc\nfrom juliacall import Main as jl\n\njl.seval('using MLPGradientFlow')\n\nmg = jl.MLPGradientFlow\n\nw = np.random.normal(size = (5, 2))/10\nb1 = np.zeros(5)\na = np.random.normal(size = (1, 5))/5\nb2 = np.zeros(1)\ninp = np.random.normal(size = (2, 10_000))\n\nw_teach = np.random.normal(size = (4, 2))\na_teach = np.random.normal(size = (1, 4))\ntarget = np.matmul(a_teach, jl.map(mg.sigmoid, np.matmul(w_teach, inp)))\n\nmg.train._jl_help() # look at the docstring\n\n# continue as in julia (see above), e.g.\np = mg.params((w, b1), (a, b2))\n\nn = mg.Net(layers = ((5, mg.sigmoid, True), (1, jl.identity, True)),\n           input = inp, target = target)\n\nres = mg.train(n, p,\n               maxtime_ode = 2, maxtime_optim = 2,\n               maxiterations_optim = 10**3, verbosity = 1)\n\n# convert the result to a python dictionary with numpy arrays\ndef convert2py(jldict):\n     d = dict(jldict)\n     for k, v in jldict.items():\n         if isinstance(v, jc.DictValue):\n             d[k] = convert2py(v)\n         if isinstance(v, jc.ArrayValue):\n             d[k] = v.to_numpy()\n     return d\n\npy_res = convert2py(res)\n\n# convert parameters in python format back to julia parameters\np = mg.params(jc.convert(jl.Dict, py_res['x']))\n\n# save results in torch.pickle format\nmg.pickle(\"myfilename.pt\", res)\n\nmg.hessian_spectrum(n, p)    # look at hessian spectrum\n\n# an MLP with 2 hidden layers with biases in the second hidden layer\nn2 = mg.Net(layers = ((5, mg.sigmoid, False), (4, mg.g, True), (2, mg.identity, False)),\n            input = inp, target = np.random.normal(size = (2, 10_000)))\n\np2 = mg.params((w, None),\n               (np.random.normal(size = (4, 5)), np.zeros(4)),\n               (np.random.normal(size = (2, 4)), None))\n\nmg.loss(n2, p2)\nmg.gradient(n2, p2)\n","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"In the following example, we train a network with one hidden layer of 5 softplus neurons on random in- and output.","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"using MLPGradientFlow, Random\n\nRandom.seed!(123)\n\ninput = randn(2, 1_000)  # 2-dimensional random input\ntarget = randn(1, 1_000) # 1-dimensional random output\n\nnet = Net(; layers = ((5, softplus, true),     # 5 relu neurons with biases\n                      (1, identity, true)), # 1 identity neuron with bias\n            input,\n            target)\n\np = random_params(net)\n\nresult = train(net, p, maxtime_ode = 20., maxtime_optim = 20., n_samples_trajectory = 10^3)","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"We see that optimization has found a point with a very small gradient:","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"gradient(net, params(result[\"x\"]))","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"Let us inspect the spectrum of the hessian:","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"hessian_spectrum(net, params(result[\"x\"]))","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"The eigenvalues are all positive, indicating that we are in a local minimum.","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"using CairoMakie\n\nfunction plot_losscurve(result; kwargs...)\n    f = Figure()\n    ax = Axis(f[1, 1], yscale = log10, xscale = log10, ylabel = \"loss\", xlabel = \"time\", kwargs...)\n    scatter!(ax, collect(keys(result[\"trajectory\"])) .+ 1, result[\"loss_curve\"])\n    f\nend\n\nplot_losscurve(result)\nsave(\"loss_curve1.png\", ans); nothing # hide","category":"page"},{"location":"train/","page":"Training to convergence","title":"Training to convergence","text":"(Image: )","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"MLPGradientFlow.jl allows to investigate the loss landscape and training dynamics of multi-layer perceptrons.","category":"page"},{"location":"#Features","page":"Overview","title":"Features","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Train multi-layer perceptrons on the CPU to convergence, using first and second order optimization methods.\nFast implementations of gradients and hessians.\nFollow gradient flow (using differential equation solvers) or popular (stochastic) gradient descent dynamics (Adam etc.).\nAccurate approximations of loss function and its derivatives for infinite normally distributed input data, using Gauss-Hermite quadrature or symbolic integrals.\nUtility functions to investigate teacher-student setups and loss landscape visualization.","category":"page"},{"location":"docstrings/#Constructing-Networks","page":"Docstrings","title":"Constructing Networks","text":"","category":"section"},{"location":"docstrings/#MLPGradientFlow.Net","page":"Docstrings","title":"MLPGradientFlow.Net","text":"Net(; layers, input, target, weights = nothing,\n      bias_adapt_input = true, derivs = 2, copy_input = true, verbosity = 1,\n      Din = size(input, 1) - last(first(layers))*(1-bias_adapt_input))\n\nlayers # ((num_neurons_layer1, activation_function_layer1, has_bias_layer1),\n       #  (num_neurons_layer2, activation_function_layer2, has_bias_layer2),\n       #   ...)\ninput  # Dᵢₙ × N matrix\ntarget # Dₒᵤₜ × N matrix\nweights # nothing or N array\nbias_adapt_input = true # adds a row of 1s to the input\nderivs = 2              # allocate memory for derivs derivatives (0, 1, 2)\ncopy_input = true       # copy the input when creating the net\n\nExample\n\njulia> input = randn(2, 100)\njulia> target = randn(1, 100)\njulia> net = Net(; layers = ((10, softplus, true), (1, identity, true)),\n                   input, target);\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MLPGradientFlow.TeacherNet","page":"Docstrings","title":"MLPGradientFlow.TeacherNet","text":"TeacherNet(; p = nothing, kwargs...)\n\nCreates a network with parameters p attached. If p == nothing, random_params are generated. A TeacherNet is a callable object that returns the target given some input. Keyword arguments kwargs are passed to Net.\n\nExample\n\njulia> teacher = TeacherNet(; layers = ((8, softplus, true), (1, identity, true)),\n                              Din = 3);\n\njulia> input = randn(3, 10^4);\n\njulia> target = teacher(input);\n\njulia> new_input = randn(3, 10^3);\n\njulia> new_target = teacher(new_input);\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MLPGradientFlow.NetI","page":"Docstrings","title":"MLPGradientFlow.NetI","text":"NetI(teacher, student; T = eltype(student.input),\n     g1 = _stride_arrayize(NormalIntegral(d = 1)),\n     g2 = _stride_arrayize(NormalIntegral(d = 2)))\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MLPGradientFlow.gauss_hermite_net","page":"Docstrings","title":"MLPGradientFlow.gauss_hermite_net","text":"gauss_hermite_net(target_function, net::Net; kwargs...)\n\nCreate from net a network with input points and weights obtained from NormalIntegral, to which kwargs are passed. The target_function is used to compute the target values. Note that in more than 2 input dimensions the number of points is excessively large (with default settings for NormalIntegral more than a million points are generated in 3 dimensions).\n\nExample\n\njulia> net = gauss_hermite_net(x -> reshape(x[1, :] .^ 2, 1, :),\n                               Net(layers = ((5, softplus, false),\n                                             (1, identity, true)), Din = 2))\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#Loss-and-its-Derivatives","page":"Docstrings","title":"Loss and its Derivatives","text":"","category":"section"},{"location":"docstrings/#MLPGradientFlow.loss","page":"Docstrings","title":"MLPGradientFlow.loss","text":"loss(net, x, input = net.input, target = net.target;\n     verbosity = 1, losstype = MSE(), weights = net.weights, maxnorm = Inf,\n     merge = nothing)\n\nCompute the loss of net at parameter value x.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.gradient","page":"Docstrings","title":"MLPGradientFlow.gradient","text":"gradient(net, x; input = net.input, target = net.target, kwargs...)\n\nCompute the gradient of net at parameter value x. See loss for kwargs.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.hessian","page":"Docstrings","title":"MLPGradientFlow.hessian","text":"hessian(net, x; input = net.input, target = net.target, kwargs...)\n\nCompute hessian of net at parameter value x. See loss for kwargs.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.hessian_spectrum","page":"Docstrings","title":"MLPGradientFlow.hessian_spectrum","text":"hessian_spectrum(net, x; kwargs...)\n\nCompute the spectrum of the hessian of net at x. Keyword arguments are passed to hessian.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#Training","page":"Docstrings","title":"Training","text":"","category":"section"},{"location":"docstrings/#MLPGradientFlow.train","page":"Docstrings","title":"MLPGradientFlow.train","text":"train(net, x0; kwargs...)\n\nTrain net from initial point x0.\n\nKeyword arguments:\n\nmaxnorm = Inf                  # constant c in loss formula\n\nbatchsize = nothing,           # using the full data set in each step when `nothing`\n\nalg = alg_default(p)           # ODE solver: KenCarp58() for length(p) ≤ 64, RK4() otherwise\nmaxT = 1e10                    # upper integration limit\nsave_everystep = true          # return a trajectory and loss curve\nn_samples_trajectory = 100     # number of samples of the trajectory\nabstol = 1e-6                  # absolute tolerance of the ODE solver\nreltol = 1e-3                  # relative tolerance of the ODE solver\nmaxtime_ode = 3*60             # maximum amount of time in seconds for the ODE solver\nmaxiterations_ode = 10^6       # maximum iterations of ODE solver\n\nmaxiterations_optim = 10^5     # maximum iterations of optimizer\nmin_gnorm = 1e-15              # stop if (the regularized) gradient ∞-norm is below min_gnorm\npatience = 10^6                # Number of steps without decrease of the loss function until converged\ntauinv = nothing               # nothing, a scalar or a ComponentArray of shape `x0` with inverse time scales\nminloss = 2e-32                # stop if MSE loss is below minloss\nmaxtime_optim = 2*60           # maximum amount of time in seconds for the Newton optimization\noptim_solver = optim_solver_default(p) # optimizer: NewtonTrustRegion() for length(p) ≤ 32, :LD_SLSQP for length(p) ≤ 1000, BFGS() otherwise\n\nverbosity = 1                  # use verbosity = 1 for more outputs\nshow_progress = true           # show progress\nprogress_interval = 5          # show progress every x seconds\nresult = :dict                 # change to result = :raw for more detailed results\nexclude = String[]             # dictionary keys to exclude from the results dictionary\ninclude = nothing              # dictionary keys to include (everything if nothing)\n\nRuns training dynamics on net from initial point x0. If an Optimisers method, e.g. Adam() or Descent() is chosen for alg, this is the time-discrete dynamics xₜ = tauinv * ∇ loss(net, xₜ₋₁) where loss(net, x) = sum((net(x) - net.target).^2) + R(x) with R(x) = 1/3 * (weightnorm(x) - maxnorm)^3 * I(weightnorm(x) > maxnorm) * n_samples(net). For ODE solvers, this is the continuous ordinary differential equation ẋ = tauinv ∇ loss(net, x), which is integrated from x(t = 0) = p to x(t = maxT). Note that maxT refers to time of the differential equation, whereas maxtime_ode (and maxtime_optim) refers to the amount of wall-clock time given to the computer to run the algorithms.\n\nIf maxiterations_optim > 0, the result of this dynamics is given to a (second order) optimizer, to find accurately the nearest minimum.\n\nAll gradient norms (min_gnorm, and return values gnorm and gnorm_regularized)  are measured in the infinity norm.\n\n\n\n\n\ntrain(net, x0::Tuple; num_workers = min(length(x0), Sys.CPU_THREADS),\n                      kwargs...)\n\nTrain from multiple initial points in parallel. kwargs are passed to the train function for each initial point.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#Utilities","page":"Docstrings","title":"Utilities","text":"","category":"section"},{"location":"docstrings/#MLPGradientFlow.random_params","page":"Docstrings","title":"MLPGradientFlow.random_params","text":"random_params(net; kwargs...)\n\n\n\n\n\nrandom_params(rng, net; distr_fn = glorot_normal)\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.params","page":"Docstrings","title":"MLPGradientFlow.params","text":"params((w₁, b₁), (w₂, b₂), ...)\n\nWhere wᵢ is a weight matrix and bᵢ is a bias vector or nothing (None in python).\n\n\n\n\n\nparams(layers::AbstractDict)\n\nConverts parameters in dictionary form to ComponentArray form.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.params2dict","page":"Docstrings","title":"MLPGradientFlow.params2dict","text":"params2dict(p)\n\nConvert a ComponentArray parameter to a dictionary\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.NormalIntegral","page":"Docstrings","title":"MLPGradientFlow.NormalIntegral","text":"NormalIntegral(; N = 600, d = 1, prune = true, threshold = 1e-18)\n\nCallable struct for Gauss-Hermite integration. Uses FastGaussianQuadrature.jl.\n\nExample\n\njulia> integrator = NormalIntegral(d = 1);\n\njulia> integrator(x -> 1.) # normalization of standard normal\n0.9999999999999998\n\njulia> integrator(identity) # mean of standard normal\n7.578393534606704e-19\n\njulia> integrator(x -> x^2) # variance of standard normal\n0.9999999999999969\n\njulia> integrator2d = NormalIntegral(d = 2);\n\njulia> integrator2d(x -> cos(x[1] + x[2])) # integrate some 2D function for x[1] and x[2] iid standard normal\n0.3678794411714446\n\njulia> integrator2d(x -> cos(x[1] + x[2]), .5) # integrate with correlation(x[1], x[2]) = 0.5\n0.22313016014843348\n\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MLPGradientFlow.trajectory_distance","page":"Docstrings","title":"MLPGradientFlow.trajectory_distance","text":"trajectory_distance(res1, res2)\n\n\n\n\n\ntrajectory_distance(trajectory, reference_trajectory)\n\nSearches for the closest points of trajectory in reference_trajectory and returns the distances, time points of the reference_trajectory and the indices in reference_trajectory.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.LinearSubspace","page":"Docstrings","title":"MLPGradientFlow.LinearSubspace","text":"LinearSubspace(ref, v1, v2)\n\nConstruct a 2D linear subspace from ref point in directions v1 and v2. See also subspace_minloss\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MLPGradientFlow.to_local_coords","page":"Docstrings","title":"MLPGradientFlow.to_local_coords","text":"to_local_coords(ls::LinearSubspace, p)\n\nProject point p to the linear subspace ls.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.subspace_minloss","page":"Docstrings","title":"MLPGradientFlow.subspace_minloss","text":"subspace_minloss(net, ref, v1, v2, a1, a2)\n\nMinimize loss in the subspace orthogonal to v1 and v2 with the point in the 2D subspace fixed to ref + a1 * v1 + a2 * v2.\n\n\n\n\n\nsubspace_minloss(net, ls::LinearSubspace, a1, a2)\n\nMinimize loss in the subspace orthogonal to ls with the point in ls fixed to ls.ref + a1 * ls.v1 + a2 * ls.v2.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.grow_net","page":"Docstrings","title":"MLPGradientFlow.grow_net","text":"grow_net(net)\n\nAdd one neuron to the hidden layer. Works only for networks with a single hidden layer.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.shrink_net","page":"Docstrings","title":"MLPGradientFlow.shrink_net","text":"shrink_net(net)\n\nRemove one neuron from the hidden layer. Works only with networks with a single hidden layer.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.split_neuron","page":"Docstrings","title":"MLPGradientFlow.split_neuron","text":"split_neuron(p, i, γ, j = i+1)\n\nDuplicate hidden neuron i with mixing ratio γ and insert the new neuron at position j. Works only for the parameters p of a network with a single hidden layer.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.cosine_similarity","page":"Docstrings","title":"MLPGradientFlow.cosine_similarity","text":"cosine_similarity(x, y) = x'*y/(norm(x)*norm(y))\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#Saving","page":"Docstrings","title":"Saving","text":"","category":"section"},{"location":"docstrings/#MLPGradientFlow.pickle","page":"Docstrings","title":"MLPGradientFlow.pickle","text":"pickle(filename, result; exclude = String[])\n\nSave the result of training in filename in torch.pickle format. See also result2dict.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MLPGradientFlow.unpickle","page":"Docstrings","title":"MLPGradientFlow.unpickle","text":"unpickle(filename)\n\nLoads the results saved with the above function pickle(filename, result; exclude = String[]).\n\n\n\n\n\n","category":"function"}]
}
